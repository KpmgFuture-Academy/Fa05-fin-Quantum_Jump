# api/news_api.py (ì•ˆì „í•œ ë²„ì „)
from fastapi import APIRouter, HTTPException, Query
from typing import List, Dict, Optional
from datetime import datetime
import json
from pathlib import Path
import pandas as pd

from services.database_service import get_database_service

router = APIRouter()

CSV_FILE_PATH = Path(__file__).parent.parent / "data" / "Past_news.csv"
df_past_news = None

def load_csv_data():
    """ì„œë²„ ì‹œì‘ ì‹œ CSV íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³ , ì»¬ëŸ¼ëª…ì„ í‘œì¤€í™”í•˜ëŠ” í•¨ìˆ˜"""
    global df_past_news
    try:
        if not CSV_FILE_PATH.is_file():
            raise FileNotFoundError(f"{CSV_FILE_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        df = pd.read_csv(CSV_FILE_PATH)
        
        print("âœ… Past_news.csvì—ì„œ ì›ë³¸ ê·¸ëŒ€ë¡œ ì½ì€ ì»¬ëŸ¼ëª…:", df.columns.tolist())

        # --- â–¼â–¼â–¼ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: ì‹¤ì œ CSV ì»¬ëŸ¼ëª…ì„ ì½”ë“œ í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë§¤í•‘ â–¼â–¼â–¼ ---
        # ì‹¤ì œ CSV íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì„ ì™¼ìª½ì—, ì½”ë“œì—ì„œ ì‚¬ìš©í•  ì´ë¦„ì„ ì˜¤ë¥¸ìª½ì— ì ìŠµë‹ˆë‹¤.
        column_mapping = {
            'ID': 'id',
            'Issue_name': 'title',
            'Contents': 'summary',
            'Contentes(Spec)': 'content', # CSV íŒŒì¼ì˜ ì˜¤íƒ€('Contentes')ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜
            'Start_date': 'start_date',
            'Fin_date': 'end_date',
            'ê·¼ê±°ìë£Œ': 'evidence_source',
            'ì¹´í…Œê³ ë¦¬': 'related_industries'
        }
        df.rename(columns=column_mapping, inplace=True)
        # --- â–²â–²â–² í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ë â–²â–²â–² ---
        
        df = df.fillna('')
        if 'id' not in df.columns or df['id'].astype(str).duplicated().any():
            df['id'] = df.index.astype(str)
        
        # 'source' ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
        if 'source' not in df.columns:
            df['source'] = 'ê³¼ê±° ì´ìŠˆ DB'

        df_past_news = df
        print(f"âœ… Past_news.csv ë°ì´í„° í‘œì¤€í™” ë° ë¡œë“œ ì„±ê³µ. ì´ {len(df_past_news)}ê°œ ë‰´ìŠ¤.")
        print("   -> ì½”ë“œì—ì„œ ì‚¬ìš©í•  ì»¬ëŸ¼ëª…:", df_past_news.columns.tolist())

    except Exception as e:
        df_past_news = pd.DataFrame()
        print(f"âŒ Past_news.csv íŒŒì¼ ë¡œë“œ/ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

load_csv_data()

@router.get("/latest")
async def get_latest_news_issues():
    """ìµœì‹  ë‰´ìŠ¤ ì´ìŠˆë“¤ì„ MySQLì—ì„œ ì¡°íšŒí•˜ê³  RAG ë¶„ì„ ìƒì„¸ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."""
    try:
        db_service = get_database_service()
        
        news_issues = await db_service.get_latest_news_issues()
        
        if not news_issues:
            # MySQLì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ fallback: ìµœì‹  JSON íŒŒì¼ì—ì„œ ë¡œë“œ
            fallback_data = _load_fallback_data()
            if fallback_data:
                # ë°±ì—… ë°ì´í„°ì—ë„ ìƒì„¸ ì •ë³´ ì¶”ê°€
                enriched_fallback = _enrich_with_rag_details(fallback_data)
                return {
                    "success": True,
                    "data": {
                        "issues": enriched_fallback,
                        "count": len(enriched_fallback),
                        "source": "íŒŒì¼ ë°±ì—… ë°ì´í„°",
                        "last_updated": "ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ ëŒ€ê¸° ì¤‘"
                    }
                }
            else:
                return {
                    "success": True,
                    "data": {
                        "issues": [],
                        "count": 0,
                        "source": "ë°ì´í„° ì—†ìŒ",
                        "message": "ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ì´ ì²« ì‹¤í–‰ì„ ì™„ë£Œí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."
                    }
                }
        
        # MySQL ë°ì´í„°ì— RAG ìƒì„¸ ì •ë³´ ì¶”ê°€
        enriched_issues = _enrich_with_rag_details(news_issues)
        
        return {
            "success": True,
            "data": {
                "issues": enriched_issues,
                "count": len(enriched_issues),
                "source": "MySQL ì‹¤ì‹œê°„ ë°ì´í„°",
                "last_updated": news_issues[0].get("updated_at") if news_issues else None,
                # ì¶”ê°€: RAG ë¶„ì„ ë©”íƒ€ë°ì´í„°
                "rag_metadata": {
                    "verification_enabled": True,
                    "confidence_calculation": "multi_dimensional",
                    "scoring_method": "hybrid_vector_ai"
                }
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")

@router.get("/past", summary="ê³¼ê±° ë‰´ìŠ¤ ëª©ë¡ ì¡°íšŒ(CSV ê¸°ë°˜)", description="data/Past_news.csv íŒŒì¼ì—ì„œ ê³¼ê±° ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
async def get_past_news(
    limit: int = 100,
    search: Optional[str] = Query(None, description="ë‰´ìŠ¤ ì œëª© ë˜ëŠ” ë‚´ìš©ì—ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œ"),
    industry: Optional[str] = Query(None, description="ê´€ë ¨ ì‚°ì—…ë³„ë¡œ í•„í„°ë§")
):
    global df_past_news
    
    if df_past_news is None or df_past_news.empty:
        raise HTTPException(status_code=500, detail="ì„œë²„ì— ê³¼ê±° ë‰´ìŠ¤ ë°ì´í„°(CSV)ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        df_filtered = df_past_news.copy()

        if search:
            search_term = search.lower()
            df_filtered = df_filtered[
                df_filtered['title'].str.lower().str.contains(search_term, na=False) |
                df_filtered['summary'].str.lower().str.contains(search_term, na=False)
            ]

        if industry:
            df_filtered = df_filtered[
                df_filtered['related_industries'].str.contains(industry, na=False)
            ]

        total_count = len(df_filtered)
        df_result = df_filtered.head(limit)
        data_to_return = df_result.to_dict(orient='records')

        return {
            "success": True,
            "total": total_count,
            "data": data_to_return
        }
    except Exception as e:
        print(f"âŒ ê³¼ê±° ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def _enrich_with_rag_details(issues: List[Dict]) -> List[Dict]:
    """ì´ìŠˆ ë°ì´í„°ì— RAG ë¶„ì„ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    enriched = []
    
    for issue in issues:
        enriched_issue = issue.copy()
        
        # ğŸ”¥ ì•ˆì „í•œ ê´€ë ¨ ì‚°ì—… ìƒì„¸ ì •ë³´ ì¶”ê°€
        raw_industries = issue.get("ê´€ë ¨ì‚°ì—…", [])
        if isinstance(raw_industries, list):
            detailed_industries = []
            for industry in raw_industries:
                if isinstance(industry, dict):
                    detailed_industry = {
                        "name": industry.get("name", "ì‚°ì—…ëª… ì—†ìŒ"),
                        "final_score": industry.get("final_score", 0),
                        "vector_score": industry.get("vector_score", 0),
                        "ai_score": industry.get("ai_score", 0),
                        "ai_reason": industry.get("ai_reason", ""),
                        "description": industry.get("description", ""),
                        # ê²€ì¦ ì •ë³´ ì•ˆì „í•˜ê²Œ ì¶”ê°€
                        "verification": industry.get("verification", {
                            "is_grounded": False,
                            "supporting_quote": ""
                        }),
                        # ì ìˆ˜ êµ¬ì„± ìƒì„¸
                        "score_breakdown": {
                            "vector_weight": 0.3,
                            "ai_weight": 0.7,
                            "penalty_applied": not industry.get("verification", {}).get("is_grounded", True)
                        }
                    }
                    detailed_industries.append(detailed_industry)
                else:
                    # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì¸ ê²½ìš° ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜
                    detailed_industries.append({
                        "name": str(industry),
                        "final_score": 0,
                        "vector_score": 0,
                        "ai_score": 0,
                        "ai_reason": "êµ¬ì¡° ë³€í™˜ë¨",
                        "description": "",
                        "verification": {"is_grounded": False, "supporting_quote": ""},
                        "score_breakdown": {"vector_weight": 0.3, "ai_weight": 0.7, "penalty_applied": True}
                    })
            enriched_issue["ê´€ë ¨ì‚°ì—…_ìƒì„¸"] = detailed_industries
        
        # ğŸ”¥ ì•ˆì „í•œ ê´€ë ¨ ê³¼ê±° ì´ìŠˆ ìƒì„¸ ì •ë³´ ì¶”ê°€
        raw_past_issues = issue.get("ê´€ë ¨ê³¼ê±°ì´ìŠˆ", [])
        if isinstance(raw_past_issues, list):
            detailed_past_issues = []
            for past_issue in raw_past_issues:
                if isinstance(past_issue, dict):
                    detailed_past_issue = {
                        "name": past_issue.get("name", "ì´ìŠˆëª… ì—†ìŒ"),
                        "final_score": past_issue.get("final_score", 0),
                        "vector_score": past_issue.get("vector_score", 0),
                        "ai_score": past_issue.get("ai_score", 0),
                        "ai_reason": past_issue.get("ai_reason", ""),
                        "description": past_issue.get("description", ""),
                        "period": past_issue.get("period", "N/A"),
                        # ê²€ì¦ ì •ë³´ ì•ˆì „í•˜ê²Œ ì¶”ê°€
                        "verification": past_issue.get("verification", {
                            "is_grounded": False,
                            "supporting_quote": ""
                        }),
                        # ì ìˆ˜ êµ¬ì„± ìƒì„¸
                        "score_breakdown": {
                            "vector_weight": 0.3,
                            "ai_weight": 0.7,
                            "penalty_applied": not past_issue.get("verification", {}).get("is_grounded", True)
                        }
                    }
                    detailed_past_issues.append(detailed_past_issue)
                else:
                    # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì¸ ê²½ìš° ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜
                    detailed_past_issues.append({
                        "name": str(past_issue),
                        "final_score": 0,
                        "vector_score": 0,
                        "ai_score": 0,
                        "ai_reason": "êµ¬ì¡° ë³€í™˜ë¨",
                        "description": "",
                        "period": "N/A",
                        "verification": {"is_grounded": False, "supporting_quote": ""},
                        "score_breakdown": {"vector_weight": 0.3, "ai_weight": 0.7, "penalty_applied": True}
                    })
            enriched_issue["ê´€ë ¨ê³¼ê±°ì´ìŠˆ_ìƒì„¸"] = detailed_past_issues
        
        # ğŸ”¥ ì•ˆì „í•œ RAG ì‹ ë¢°ë„ ìƒì„¸ ì •ë³´ ì¶”ê°€
        rag_confidence = issue.get("RAGë¶„ì„ì‹ ë¢°ë„", {})
        if isinstance(rag_confidence, dict):
            consistency_score = rag_confidence.get("consistency_score", 0)
            peak_relevance_score = rag_confidence.get("peak_relevance_score", 0)
        elif isinstance(rag_confidence, (int, float)):
            # êµ¬ ë²„ì „ í˜¸í™˜
            consistency_score = float(rag_confidence)
            peak_relevance_score = float(rag_confidence)
        else:
            consistency_score = 0
            peak_relevance_score = 0
        
        enriched_issue["RAGë¶„ì„ì‹ ë¢°ë„_ìƒì„¸"] = {
            "consistency_score": consistency_score,
            "peak_relevance_score": peak_relevance_score,
            "calculation_method": "í‰ê·  ì¼ê´€ì„± + ìµœê³  ì—°ê´€ë„",
            "total_verified_items": sum(1 for ind in detailed_industries 
                                      if ind.get("verification", {}).get("is_grounded", False)) +
                                  sum(1 for past in detailed_past_issues 
                                      if past.get("verification", {}).get("is_grounded", False))
        }
        
        enriched.append(enriched_issue)
    
    return enriched

def _load_fallback_data():
    """JSON íŒŒì¼ì—ì„œ ë°±ì—… ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤ (ì•ˆì „í•œ ë²„ì „)."""
    try:
        data_dir = Path("data2")
        if not data_dir.exists():
            return []
        
        # ê°€ì¥ ìµœê·¼ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
        pipeline_files = list(data_dir.glob("*Pipeline_Results.json"))
        if not pipeline_files:
            return []
        
        latest_file = max(pipeline_files, key=lambda p: p.stat().st_mtime)
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ğŸ”¥ ë‹¤ì–‘í•œ íŒŒì¼ êµ¬ì¡° ì²˜ë¦¬
        issues = []
        
        # ìƒˆë¡œìš´ êµ¬ì¡°: {"selected_issues": [...]}
        if "selected_issues" in data:
            issues = data["selected_issues"]
        # êµ¬ API êµ¬ì¡°: {"api_ready_data": {"data": {"selected_issues": [...]}}}
        elif "api_ready_data" in data:
            api_data = data.get("api_ready_data", {})
            issues = api_data.get("data", {}).get("selected_issues", [])
        
        return issues if isinstance(issues, list) else []
    
    except Exception as e:
        print(f"ë°±ì—… ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

@router.get("/pipeline-status")
async def get_pipeline_status():
    """ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ì˜ ìµœê·¼ ì‹¤í–‰ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        db_service = get_database_service()
        latest_log = await db_service.get_latest_pipeline_log()
        
        return {
            "success": True,
            "data": latest_log or {
                "status": "ëŒ€ê¸° ì¤‘",
                "message": "ë°±ê·¸ë¼ìš´ë“œ íŒŒì´í”„ë¼ì¸ì´ ì•„ì§ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")