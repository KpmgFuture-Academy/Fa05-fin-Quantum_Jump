"""
íŒŒì´í”„ë¼ì¸ í†µí•© ì„œë¹„ìŠ¤ - í¬ë¡¤ë§, í•„í„°ë§, RAG ë¶„ì„ì„ ì—°ê²°
integrated_pipeline.pyì˜ IntegratedNewsPipeline ë¡œì§ ì´ê´€ (ìˆ˜ì • ì‚¬í•­ ë°˜ì˜)
ì™„ì „ ì•ˆì „ ë²„ì „ - ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬
"""

import json
import traceback  # ì˜¤ë¥˜ ì¶”ì ì„ ìœ„í•´ ì¶”ê°€
from pathlib import Path
from typing import Dict, List
from datetime import datetime
from .crawling_service import CrawlingService
from .rag_service import RAGService

class PipelineService:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ì„œë¹„ìŠ¤"""
    
    def __init__(self, data_dir: str = "data2", headless: bool = True):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.crawling_service = CrawlingService(str(self.data_dir), headless)
        self.rag_service = RAGService()
        
        print("âœ… íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def execute_full_pipeline(self, 
                                issues_per_category: int = 10,
                                target_filtered_count: int = 5) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: í¬ë¡¤ë§ â†’ í•„í„°ë§ â†’ RAG ë¶„ì„ (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)"""
        
        pipeline_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        started_at = datetime.now()
        
        print(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ (ID: {pipeline_id})")
        print(f"ğŸ“‹ ì„¤ì •: ì¹´í…Œê³ ë¦¬ë³„ {issues_per_category}ê°œ, ìµœì¢… ì„ ë³„ {target_filtered_count}ê°œ")
        
        result = {
            "pipeline_id": pipeline_id,
            "started_at": started_at.strftime("%Y-%m-%d %H:%M:%S"),
            "completed_at": None,
            "execution_time": None,
            "final_status": "running",
            "steps_completed": [],
            "errors": []
        }
        
        try:
            # Step 1: í¬ë¡¤ë§ + í•„í„°ë§
            print(f"\n{'='*60}")
            print(f"ğŸ“¡ Step 1: í¬ë¡¤ë§ + ì£¼ì‹ì‹œì¥ í•„í„°ë§")
            print(f"{'='*60}")
            
            crawling_result = self.crawling_service.crawl_and_filter_news(
                issues_per_category, target_filtered_count
            )
            
            result["crawling_result"] = {
                "total_crawled": len(crawling_result.get("all_issues", [])),
                "filtered_count": len(crawling_result.get("filtered_issues", []))
            }
            result["steps_completed"].append("crawling_and_filtering")
            
            # Step 2: RAG ë¶„ì„ (ì™„ì „ ì•ˆì „ ë²„ì „)
            print(f"\n{'='*60}")
            print(f"ğŸ” Step 2: RAG ë¶„ì„ (ì‚°ì—… + ê³¼ê±° ì´ìŠˆ)")
            print(f"{'='*60}")
            
            # ğŸ”¥ í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ í•„í„°ë§ëœ ì´ìŠˆ ì¶”ì¶œ (ë‹¤ì–‘í•œ êµ¬ì¡° ì²˜ë¦¬)
            raw_filtered = crawling_result.get("filtered_issues", [])
            print(f"ğŸ“Š ì›ë³¸ í•„í„°ë§ ê²°ê³¼ íƒ€ì…: {type(raw_filtered)}")
            print(f"ğŸ“Š ì›ë³¸ í•„í„°ë§ ê²°ê³¼ ê¸¸ì´: {len(raw_filtered) if hasattr(raw_filtered, '__len__') else 'N/A'}")
            
            # ğŸ”¥ ë‹¤ì–‘í•œ êµ¬ì¡° ì²˜ë¦¬
            if isinstance(raw_filtered, dict) and "selected_issues" in raw_filtered:
                # StockFiltered íŒŒì¼ êµ¬ì¡°: {"selected_issues": [...]}
                filtered_issues = raw_filtered["selected_issues"]
                print(f"ğŸ“‹ selected_issuesì—ì„œ ì¶”ì¶œ: {len(filtered_issues)}ê°œ")
            elif isinstance(raw_filtered, list) and raw_filtered:
                if isinstance(raw_filtered[0], dict):
                    # ì´ë¯¸ ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬ ë°°ì—´: [{"ì œëª©": "...", "ë‚´ìš©": "..."}, ...]
                    filtered_issues = raw_filtered
                    print(f"ğŸ“‹ ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬ ë°°ì—´: {len(filtered_issues)}ê°œ")
                    print(f"ğŸ“‹ ì²« ë²ˆì§¸ ì´ìŠˆ í‚¤ë“¤: {list(raw_filtered[0].keys())}")
                else:
                    # ë¬¸ìì—´ ë°°ì—´ â†’ ë”•ì…”ë„ˆë¦¬ ë°°ì—´ ë³€í™˜: ["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2", ...]
                    print("âš™ï¸ ë¬¸ìì—´ ë°°ì—´ì„ ë”•ì…”ë„ˆë¦¬ ë°°ì—´ë¡œ ë³€í™˜...")
                    filtered_issues = []
                    for i, text in enumerate(raw_filtered):
                        filtered_issues.append({
                            "ì´ìŠˆë²ˆí˜¸": i + 1,
                            "ì œëª©": f"í•„í„°ë§ëœ ì´ìŠˆ {i+1}",
                            "ë‚´ìš©": str(text),
                            "ì›ë³¸ë‚´ìš©": str(text),
                            "ì¹´í…Œê³ ë¦¬": "ìë™ë³€í™˜",
                            "ì¶”ì¶œì‹œê°„": datetime.now().isoformat(),
                            "ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜": 5.0,
                            "rank": i + 1
                        })
                    print(f"âœ… {len(filtered_issues)}ê°œ ì´ìŠˆ ë³€í™˜ ì™„ë£Œ")
            else:
                raise ValueError("í•„í„°ë§ëœ ì´ìŠˆê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            if not filtered_issues:
                raise ValueError("ë³€í™˜ëœ í•„í„°ë§ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # RAG ë¶„ì„ ì‹¤í–‰
            print(f"ğŸ” RAG ë¶„ì„ ì‹œì‘ - {len(filtered_issues)}ê°œ ì´ìŠˆ ì²˜ë¦¬...")
            enriched_issues = self.rag_service.analyze_issues_with_rag(filtered_issues)
            print(f"âœ… RAG ë¶„ì„ ì™„ë£Œ - {len(enriched_issues)}ê°œ ì´ìŠˆ ì²˜ë¦¬ë¨")
            
            # ğŸ”¥ [ìˆ˜ì •] ì•ˆì „í•œ í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            try:
                average_confidence = self._calculate_average_confidence(enriched_issues)
                print(f"âœ… í‰ê·  ì‹ ë¢°ë„ ê³„ì‚° ì™„ë£Œ: {average_confidence}")
            except Exception as conf_error:
                print(f"âš ï¸ í‰ê·  ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {conf_error}")
                average_confidence = 0.0

            result["rag_result"] = {
                "analyzed_count": len(enriched_issues),
                "average_confidence": average_confidence
            }
            result["steps_completed"].append("rag_analysis")
            
            # Step 3: APIìš© ë°ì´í„° ì¤€ë¹„
            print(f"\n{'='*60}")
            print(f"ğŸŒ Step 3: API ì‘ë‹µ ë°ì´í„° ì¤€ë¹„")
            print(f"{'='*60}")
            
            api_data = self._prepare_api_data(crawling_result, enriched_issues)
            result["api_ready_data"] = api_data
            result["steps_completed"].append("api_preparation")
            
            # íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
            completed_at = datetime.now()
            execution_time = completed_at - started_at
            
            result.update({
                "completed_at": completed_at.strftime("%Y-%m-%d %H:%M:%S"),
                "execution_time": str(execution_time),
                "final_status": "success",
                # ğŸ”¥ ì¶”ê°€: ìµœì¢… ê²°ê³¼ ìš”ì•½
                "final_summary": {
                    "total_issues": len(enriched_issues),
                    "average_confidence": average_confidence,
                    "processing_details": {
                        "crawled": result["crawling_result"]["total_crawled"],
                        "filtered": result["crawling_result"]["filtered_count"],
                        "analyzed": len(enriched_issues)
                    }
                }
            })
            
            # ê²°ê³¼ ì €ì¥
            saved_file = self._save_pipeline_result(result, enriched_issues)
            result["saved_file"] = saved_file
            
            print(f"\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            print(f"â° ì‹¤í–‰ ì‹œê°„: {execution_time}")
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(enriched_issues)}ê°œ ì´ìŠˆ ë¶„ì„ ì™„ë£Œ")
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {saved_file}")
            
            return result
            
        except Exception as e:
            error_msg = f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            
            # ğŸ”¥ [ìˆ˜ì •] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ë¡œê¹…
            print("ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            traceback.print_exc()

            result.update({
                "completed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "final_status": "failed",
                "errors": [error_msg, traceback.format_exc()]
            })
            
            # ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œ ìƒìœ„ í˜¸ì¶œìì—ê²Œ ì „íŒŒ
            raise Exception(error_msg)

    def _prepare_api_data(self, crawling_result: Dict, enriched_issues: List[Dict]) -> Dict:
        """API ì‘ë‹µìš© ë°ì´í„° êµ¬ì„±"""
        
        api_data = {
            "success": True,
            "data": {
                "total_crawled": len(crawling_result.get("all_issues", [])),
                "selected_count": len(enriched_issues),
                "selection_criteria": "ì£¼ì‹ì‹œì¥ ì˜í–¥ë„ + RAG ë¶„ì„",
                "selected_issues": []
            },
            "metadata": {
                "crawled_at": crawling_result.get("crawling_metadata", {}).get("timestamp", ""),
                "categories_processed": crawling_result.get("crawling_metadata", {}).get("categories_processed", []),
                "ai_filter_applied": True,
                "rag_analysis_applied": True,
                "filter_model": "gpt-4o-mini",
                "rag_model": "gpt-4o-mini",
                "rag_confidence": self._calculate_average_confidence(enriched_issues)
            }
        }
        
        # ì´ìŠˆ ë°ì´í„° ë³€í™˜
        for issue in enriched_issues:
            api_issue = {
                "ì´ìŠˆë²ˆí˜¸": issue.get("ì´ìŠˆë²ˆí˜¸", 0),
                "ì œëª©": issue.get("ì œëª©", ""),
                "ë‚´ìš©": issue.get("ì›ë³¸ë‚´ìš©", issue.get("ë‚´ìš©", "")),
                "ì¹´í…Œê³ ë¦¬": issue.get("ì¹´í…Œê³ ë¦¬", ""),
                "ì¶”ì¶œì‹œê°„": issue.get("ì¶”ì¶œì‹œê°„", ""),
                "ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜": issue.get("ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜", 0),
                "ìˆœìœ„": issue.get("rank", 0),
                
                # RAG ë¶„ì„ ê²°ê³¼
                "ê´€ë ¨ì‚°ì—…": issue.get("ê´€ë ¨ì‚°ì—…", []),
                "ê´€ë ¨ê³¼ê±°ì´ìŠˆ": issue.get("ê´€ë ¨ê³¼ê±°ì´ìŠˆ", []),
                # ğŸ”¥ [ìˆ˜ì •] RAG ì‹ ë¢°ë„ ê¸°ë³¸ê°’ì„ dictë¡œ ë³€ê²½í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
                "RAGë¶„ì„ì‹ ë¢°ë„": issue.get("RAGë¶„ì„ì‹ ë¢°ë„", {"consistency_score": 0.0, "peak_relevance_score": 0.0}),
            }
            api_data["data"]["selected_issues"].append(api_issue)
        
        # ìˆœìœ„ë³„ ì •ë ¬
        api_data["data"]["selected_issues"].sort(key=lambda x: x.get("ìˆœìœ„", 999))
        
        return api_data
    
    # ğŸ”¥ [êµì²´] ìƒˆë¡œìš´ _calculate_average_confidence ë©”ì„œë“œ
    def _calculate_average_confidence(self, enriched_issues: List[Dict]) -> float:
        """ì „ì²´ ì´ìŠˆë“¤ì˜ í‰ê·  RAG ì‹ ë¢°ë„ ê³„ì‚° (ì˜¤ë¥˜ ìˆ˜ì •ë¨)"""
        if not enriched_issues:
            return 0.0
        
        confidences = []
        
        for issue in enriched_issues:
            rag_confidence = issue.get("RAGë¶„ì„ì‹ ë¢°ë„")
            
            if rag_confidence is None:
                continue
            
            # ğŸ”¥ ì˜¤ë¥˜ ìˆ˜ì •: ë”•ì…”ë„ˆë¦¬ì™€ ìˆ«ì íƒ€ì… ëª¨ë‘ ì²˜ë¦¬
            if isinstance(rag_confidence, dict):
                # ìƒˆë¡œìš´ ë‹¤ì°¨ì› ì‹ ë¢°ë„ êµ¬ì¡°
                consistency_score = rag_confidence.get("consistency_score", 0)
                confidences.append(float(consistency_score))
            elif isinstance(rag_confidence, (int, float)):
                # ê¸°ì¡´ ë‹¨ì¼ ìˆ«ì êµ¬ì¡°
                confidences.append(float(rag_confidence))
            else:
                # ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…ì€ ê±´ë„ˆë›°ê¸°
                print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ RAG ì‹ ë¢°ë„ íƒ€ì…: {type(rag_confidence)}, ê°’: {rag_confidence}")
                continue
        
        if not confidences:
            return 0.0
            
        return round(sum(confidences) / len(confidences), 2)

    def _save_pipeline_result(self, result: Dict, enriched_issues: List[Dict]) -> str:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ (í–¥ìƒëœ ë²„ì „)"""
        try:
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_Pipeline_Results.json"
            filepath = self.data_dir / filename
            
            # ğŸ”¥ í–¥ìƒëœ ì €ì¥ ë°ì´í„° êµ¬ì¡°
            save_data = {
                "timestamp": datetime.now().isoformat(),
                "total_issues": len(enriched_issues),
                "selected_issues": enriched_issues,  # í•µì‹¬: enriched_issues ì§ì ‘ ì €ì¥
                "average_confidence": self._calculate_average_confidence(enriched_issues),
                "processing_time": result.get("execution_time", ""),
                "pipeline_metadata": {
                    "pipeline_id": result.get("pipeline_id", ""),
                    "steps_completed": result.get("steps_completed", []),
                    "final_status": result.get("final_status", ""),
                    "version": "PipelineService_v1.2_SafeVersion"
                },
                "file_info": {
                    "filename": filename,
                    "created_at": datetime.now().isoformat(),
                    "format_version": "2.0"
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return ""

    def get_latest_analyzed_issues(self) -> List[Dict]:
        """ìµœì‹  ë¶„ì„ëœ ì´ìŠˆë“¤ ì¡°íšŒ (APIìš©) - í–¥ìƒëœ ë²„ì „"""
        try:
            # 1. MySQLì—ì„œ ë¨¼ì € ì¡°íšŒ ì‹œë„
            try:
                from .database_service import DatabaseService
                db_service = DatabaseService()
                
                if db_service.is_initialized():
                    mysql_data = db_service.get_latest_news_issues()
                    if mysql_data:
                        print(f"ğŸ“Š MySQLì—ì„œ {len(mysql_data)}ê°œ ì´ìŠˆ ì¡°íšŒ")
                        return mysql_data
            except Exception as db_error:
                print(f"âš ï¸ MySQL ì¡°íšŒ ì‹¤íŒ¨: {db_error}")
            
            # 2. MySQLì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìµœì‹  íŒŒì¼ì—ì„œ ì¡°íšŒ
            pipeline_files = list(self.data_dir.glob("*_Pipeline_Results.json"))
            if pipeline_files:
                latest_file = max(pipeline_files, key=lambda f: f.stat().st_mtime)
                
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ğŸ”¥ ë‹¤ì–‘í•œ íŒŒì¼ êµ¬ì¡° ì²˜ë¦¬
                issues = []
                
                # ìƒˆë¡œìš´ êµ¬ì¡°: {"selected_issues": [...]}
                if "selected_issues" in data:
                    issues = data["selected_issues"]
                    print(f"ğŸ“‚ selected_issuesì—ì„œ ì¡°íšŒ: {len(issues)}ê°œ")
                # êµ¬ API êµ¬ì¡°: {"api_ready_data": {"data": {"selected_issues": [...]}}}
                elif "api_ready_data" in data:
                    api_data = data.get("api_ready_data", {})
                    issues = api_data.get("data", {}).get("selected_issues", [])
                    print(f"ğŸ“‚ api_ready_dataì—ì„œ ì¡°íšŒ: {len(issues)}ê°œ")
                
                if issues:
                    print(f"ğŸ“‚ íŒŒì¼ì—ì„œ ì´ìŠˆ ì¡°íšŒ ì„±ê³µ: {latest_file.name}")
                    return issues
            
            print("âš ï¸ ë¶„ì„ëœ ì´ìŠˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
        except Exception as e:
            print(f"âŒ ìµœì‹  ì´ìŠˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []